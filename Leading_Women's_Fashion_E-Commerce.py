# -*- coding: utf-8 -*-
"""Another copy of Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bpKHcNV2bjlIqnn1fCEaeVSlfrKgVNO4
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
uploaded = files.upload()
uploaded

text = pd.read_excel('Womens Clothing Reviews Data.xlsx')

text.isnull().sum()

text['Review'] = text['Review Title']+ " " +text['Review Text']

text.head(5)

text['Review'].isna().sum()*100/text.shape[0]

text['Review'] = text['Review'].fillna("")

text['Review'].head(5)

text['Review'] = text['Review'].str.lower()

import spacy

nlp = spacy.load('en_core_web_sm')

text['Review'] = text['Review'].apply(nlp)

stop_word = nlp.Defaults.stop_words

text.columns

cleaned_text = []

for token_ in text['Review']:
  filtered_text = [word.lemma_ for word in token_ if not word.is_stop and not word.is_punct and word.pos_ in  ['NOUN','VERB','ADJ'] ]
  cleaned_text.append(" ".join(filtered_text).strip())
text['Review_cleaned'] = cleaned_text

text['Review_cleaned']

#for word in text['Review']:
 # for i in word:
  #  if i.pos_ in ['NOUN','VERB','ADJ']:
  #    print(i)

text.isnull().sum()

text['Category'] = text['Category'].fillna('Not_Available')

text['Subcategory1'] = text['Subcategory1'].fillna('Not_Available')

text['SubCategory2'] = text['SubCategory2'].fillna('Not_Available')

category_count = text['Category'].value_counts()

category_count.values

figure,axes = plt.subplots(figsize = (10,5))
plt.bar(x = category_count.index,height= category_count.values)
plt.plot( category_count.index , category_count.values,color = 'red',marker = 'o')
plt.title('Distribution by Category')
plt.show()

text.columns = [col.strip() for col in text.columns ]

text.columns

text.rename(columns = {'Product ID':'Product_ID','Review Title':'Review_Title','Recommend Flag':'Recommend_Flag','Customer Age':'Customer_Age'},inplace = True)

location_grp = text.groupby('Location')['Product_ID'].count()

plt.figure(figsize = (10,5))
plt.pie(x=location_grp.values, labels  = location_grp.index,autopct= '%1.1f%%')
plt.title('Distribution of Products count on Basis of Loaction')
plt.show()

channel_grp = text.groupby('Channel')['Product_ID'].count()
channel_grp

plt.title('Channel by Product ')
plt.bar(x = channel_grp.index , height =channel_grp.values )

for i,val in enumerate(channel_grp):
  plt.text(i,val+(0.01*max(channel_grp.values)),str(val))

from collections import Counter
word_freq = Counter(" ".join(text['Review_cleaned']).split())

words_count = pd.DataFrame(word_freq.items() , columns = ['Word','Frequency'])

word_filter = words_count[words_count['Frequency']>5]['Word'].tolist()

word_filter = list(set(word_filter))

text['Review_cleaned'] = text['Review_cleaned'].apply(lambda review : " ".join ([word for word in review.split() if word in word_filter]))

text['Review_cleaned'][20]

import nltk

from nltk import FreqDist
fdist = FreqDist(text['Review_cleaned'])

fdist.items()

#polarity =[]

from textblob import TextBlob
text['Polarity'] = text['Review_cleaned'].apply(lambda x: round(TextBlob(x).sentiment.polarity,2))

text['subjectivity'] = text['Review_cleaned'].apply(lambda x: round(TextBlob(x).subjectivity,2))

text['Type'] =text['Polarity'].map(lambda x: 'Neutral' if x == 0.00 else 'Negative' if x <0.00 else 'Positive' if x<= 0.5  else 'Highly_Positive')

text['Type'].value_counts()

"""FOR POSITIVE REVIEWS ANALYSIS"""

positive_analysis = text[(text['Type'] == 'Positive') | (text['Type'] == 'Highly_Positive')]

positive_analysis.columns

spli = ' '.join(positive_analysis['Review_cleaned']).split()

fdist_ = FreqDist(spli)
#fdist_.items()
f = print(fdist_.most_common)

import wordcloud

from wordcloud import WordCloud
wc = WordCloud(width = 1200 , height = 600, background_color= 'white',min_font_size=10,max_words=200).generate_from_frequencies(fdist_)

plt.figure(figsize = (14,7))
plt.imshow(wc,interpolation='bilinear')
plt.title('Word_Cloud For Positive Reviews')

"""Negative Reviews Analysis"""

Negative_analysis = text[(text['Type'] == 'Negative')]

negative_spli = ' '.join(Negative_analysis['Review_cleaned']).split()

fdist_negative = FreqDist(negative_spli)

from wordcloud import WordCloud
wc_negative = WordCloud(width = 1200 , height = 600, background_color= 'white',min_font_size=10,max_words=200).generate_from_frequencies(fdist_negative)

from inspect import formatannotation
plt.figure(figsize = (14,7))
plt.imshow(wc_negative,interpolation='bilinear')
plt.title('Word_Cloud For Negative Reviews',fontsize =20)
plt.show()

"""Topic Mining Unsupervised Learning"""

from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer

vect = CountVectorizer()
vect_count = vect.fit_transform(text['Review_cleaned'])

vect_names = vect.get_feature_names_out()

lda_model = LatentDirichletAllocation(n_components=10,max_iter=100,random_state=101)
lda_model.fit(vect_count)

for i , names in enumerate(lda_model.components_):
  top_words = [vect_names[word] for word in names.argsort()[-10:][::-1]]
  print(f'Topic{i+1} : {", " .join(top_words)}')

1)Sale price
2)clothing size
3)skirt length
4)shirt
5)blouse
6)bra wear
7)summer dress
8)fabric
9)pant
10)winter sweater

topic_assignments = lda_model.transform(vect_count)
text['Topic'] = topic_assignments.argmax(axis = 1)

text['Topic'] = text['Topic'].map({1:'Sale_price',2:'Clothing_size',3:'Skirt_Length',4:'Shirt',5:'Blouse',6:'Bra',7:'summer dress',8:'Fabric',9:'Pant',10:'Winter_sweater'})

text.drop(columns = ['Review_Title','Review Text','Product_ID','Review'],inplace = True)

text.info()

col_alpha = []

for col in text.columns:
  if text[col].dtype  =='object'and col != 'Review_cleaned':
    col_alpha.append(col)

col_alpha

text[col_alpha]

from sklearn.preprocessing import LabelEncoder
label = LabelEncoder()
for col in text[col_alpha]:
  text[col] = label.fit_transform(text[col])

text

tfid = TfidfVectorizer()
X_tfid = tfid.fit_transform(text['Review_cleaned'])
X_tfid

text.columns

from scipy.sparse import hstack

combine_col = text[['Rating','Category','Subcategory1','SubCategory2','Location','Channel','Customer_Age','Polarity','subjectivity','Topic']]

X_text = hstack([X_tfid,combine_col])

Y_text = text['Recommend_Flag']

Y_text.value_counts()

X_text

"""Modelling"""

from sklearn.model_selection import train_test_split
Train_X,Test_X,Train_Y,Test_Y = train_test_split(X_text,Y_text,test_size = 0.2,random_state = 101)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,classification_report

log  = LogisticRegression()
log.fit(Train_X,Train_Y)

train_predict = log.predict(Train_X)
accuracy_score(train_predict,Train_Y)

train_predict_test = log.predict(Test_X)
accuracy_score(train_predict_test,Test_Y)

